---
title: "Hugging Face"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,error=TRUE,collapse = T)
library(reticulate)
use_virtualenv("/Users/peltouz/Documents/GitHub/M2-Py-DS2E/hf", required=TRUE)
py_config()
#  md_document:
#    variant: markdown_github
#
#
#<div class = "row">
#<div class = "row">
#<div class = "python">
#  
#
#</div>
#</div>
#<div class = "row">
#<div class = "r">  
#  
#  
#</div>
#</div>
#</div>


#<style>
#div.blue pre { background-color:lightblue; }
#div.blue pre.r { background-color:blue; }
#</style>
#
```

<style>
div.python pre { 
  background-color: #f5eff8; }
</style>

<style>
div.r pre { background-color: #fff5fd; }
</style>


<style>
div.exo pre { background-color: #e3e9ea; }
</style>


<p>&nbsp;</p>


# Transformers model

<p>&nbsp;</p>

## For which tasks?

- **Classifying whole sentences**:
  - Examples: Sentiment analysis, spam detection, grammatical correctness, sentence relationship.


- **Classifying each word in a sentence**:
  - Examples: Grammatical components (noun, verb, adjective), named entity recognition (person, location, organization).


- **Generating text content**:
  - Examples: Autocomplete text from a prompt, fill in masked words in a text.


- **Extracting an answer from a text**:
  - Examples: Given a question and context, extract the answer based on the context.


- **Generating a new sentence from an input text**:
  - Examples: Translation, text summarization.


<p>&nbsp;</p>

### A challenging task

- **Human vs Machine Language Processing**:
  - Humans understand sentences like "I am hungry" easily, and can compare similar sentences like "I am hungry" and "I am sad".
  - Machine learning models find it more difficult to process and understand language, requiring careful text processing.


- **Introduction of Transformers**:
  - **2017**: Introduction of Transformers by Vaswani et al. in the paper "Attention Is All You Need."
    - Neural network architecture learning context and relationships from sequential data, initially focused on translation tasks.


- **Influential Transformer Models**:
  - **June 2018**: GPT - First pretrained transformer model, state-of-the-art results on various NLP tasks.
  - **October 2018**: BERT - Designed to produce better sentence summaries.
  - **February 2019**: GPT-2 - Improved version of GPT, not immediately released due to ethical concerns.
  - **October 2019**: DistilBERT - A distilled version of BERT, 60% faster, 40% lighter, retaining 97% of BERTâ€™s performance.
  - **October 2019**: BART and T5 - Pretrained models using the original Transformer architecture.
  - **May 2020**: GPT-3 - Larger model, capable of zero-shot learning.
  - **November 2022**: GPT-3.5 and **March 2023**: GPT-4.
  - Followed by other models like **Llama**, **Claude**, **Gemini**, **Mistral**.


<p>&nbsp;</p>

# Pretraining and Fine-tuning

- **Transformer models (GPT, BERT, BART, T5, etc.)** are trained as language models on large amounts of raw text using self-supervised learning.
   - Self-supervised learning: the model learns without human-labeled data, as the objective is computed automatically from the inputs.
   - These models develop a statistical understanding of language but aren't directly useful for specific tasks.

- **Transfer learning**: the process where a pretrained model is fine-tuned on specific tasks using supervised learning (human-annotated labels).
   - Pretrained models undergo fine-tuning to adapt to particular tasks.


- **Pretraining**:
   - The model is trained from scratch with randomly initialized weights.
   - Pretraining is resource-intensive in terms of time, data, and money.
   - It requires a large corpus and can take weeks to complete.


- **Fine-tuning**:
   - Performed after pretraining using a dataset specific to a task.
   - Reasons to fine-tune instead of training from scratch:
     - The pretrained model shares similarities with the fine-tuning dataset.
     - Less data and time are required to achieve good results.
     - Lower costs in terms of time, data, financial, and environmental resources.
     - It allows for faster iteration and refinement.


- **Transfer learning advantages**:
   - Leverages knowledge from pretraining for improved task-specific performance.
   - Fine-tuning achieves better results than training from scratch unless massive data is available.
   - Using pretrained models close to the target task optimizes performance.



<p>&nbsp;</p>
# Transformers Library

- The ðŸ¤— Transformers library provides tools to create and use shared models.


- **Model Hub**:
  - Contains thousands of pretrained models for download and use.
  - Users can also upload their own models to the Hub.
  
  
- **Pipeline() function**:
  - The most basic object in the library.
  - Connects a model with necessary preprocessing and postprocessing steps.
  - Allows direct input of text for intelligible output.


- **Three main steps in a pipeline**:
  1. Text is preprocessed into a format the model understands.
  2. Preprocessed inputs are passed to the model.
  3. Model predictions are post-processed for easy interpretation.


- **Available pipelines**:
  - feature-extraction (vector representation of a text)
  - fill-mask
  - ner (named entity recognition)
  - question-answering
  - sentiment-analysis
  - summarization
  - text-generation
  - translation
  - zero-shot-classification




<div class = "python">
```{python, eval = F}
import os
hf_token = "your-key"
custom_cache_dir = '/Users/peltouz/Documents/pretrain'

os.environ['HF_HOME'] = custom_cache_dir  # Hugging Face home directory for all HF operations
os.environ['TRANSFORMERS_CACHE'] = custom_cache_dir  # Transformers-specific cache directory
os.environ['HF_DATASETS_CACHE'] = custom_cache_dir  # Datasets-specific cache directory
os.environ['HF_METRICS_CACHE'] = custom_cache_dir  # Metrics-specific cache directory
os.environ['HF_TOKEN'] = hf_token  # Hugging Face API token
```
</div>

This Python code snippet configures the environment to specify custom cache directories for operations involving Hugging Face libraries, and it sets an API token for authentication.

- **API Token Configuration**:
   - `hf_token`: Assigned to a string representing the Hugging Face API token.
   - Purpose: Used for authenticating and accessing Hugging Face services that require credentials.


- **Custom Cache Directory**:
   - `custom_cache_dir`: Set to `D:/pretrain`.
   - Purpose: Specifies a base directory for storing cache files related to Hugging Face operations.


- **Environment Variables**:
   - `HF_HOME`: Specifies the base directory for all Hugging Face-related operations.
   - `TRANSFORMERS_CACHE`: Directory for caching models downloaded via the transformers library.
   - `HF_DATASETS_CACHE`: Directory for caching datasets accessed via the datasets library.
   - `HF_METRICS_CACHE`: Directory for caching metrics-related files used in model evaluation.
   - `HF_TOKEN`: Environment variable for the Hugging Face API token to authenticate requests to Hugging Face services.


- **Purpose of Environment Variables**:
   - Ensure all data related to models, datasets, and metrics are stored in the specified directory (`D:/pretrain`).
   - Help manage disk space, especially when handling large models or datasets.
   - Correctly configure credentials for accessing restricted services.


<p>&nbsp;</p>

# The Pipeline function

<p>&nbsp;</p>

## Sentiment Analysis

<div class = "python">
```{python}
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
sentiments = classifier(
        ["I hate teaching",
         "I love programming"]
    )

print(sentiments)
sentiments[0]['label']
```
</div>

  - A pretrained model fine-tuned for sentiment analysis in English is selected by default.
  - The model is downloaded and cached when the classifier object is created.
  - Upon rerunning the command, the cached model is used without needing to download again.


<p>&nbsp;</p>

## Zero-shot classification

<div class = "python">
```{python}
classifier = pipeline("zero-shot-classification")
classifier(
    "df %>% filter(!is.na(var1))",
    candidate_labels=["python", "Rstudio"],
)
```
</div>

 - **Main idea**: 
   - Classifying unlabelled texts is a challenging task often encountered in real-world projects.


- **Comparison**: 
  - Annotating text manually is time-consuming and requires domain expertise.


- **Key aspect**: 
  - The zero-shot-classification pipeline is powerful because it lets you specify your own labels for classification, instead of relying on pretrained model labels.

<p>&nbsp;</p>

## Text generation

<div class = "python">
```{python}
generator = pipeline("text-generation")
generator("In my programming course in DS2E I will")
```
</div>

- **Main idea**: 
  - A prompt is provided, and the model auto-completes it by generating the remaining text.
  
  
- **Comparison**: 
  - Similar to the predictive text feature on phones.
  
  
- **Key aspect**: 
  - Text generation involves randomness, so results may vary

<p>&nbsp;</p>

## Mask filling

<div class = "python">
```{python}
unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```
</div>

- **Main idea**: 
  - The task involves filling in the blanks in a given text.


- **Comparison**: 
  - Similar to completing sentences in cloze tests or gap-filling exercises.
  

- **Key aspect**: 
  - The focus is on providing contextually appropriate words or phrases to complete the text.

<p>&nbsp;</p>

## Named entity recognition

<div class = "python">
```{python}
ner = pipeline("ner", grouped_entities=True)
ner("My name is Pierre and I work at BETA in Strasbourg.")
```
</div>

- **Main idea**: 
  - Named entity recognition (NER) involves identifying parts of the input text that correspond to entities.


- **Comparison**: 
  - NER focuses on entities like persons, locations, or organizations in the text.


<p>&nbsp;</p>

## Question answering

<div class = "python">
```{python}
question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Pierre and I work at BETA in Strasbourg.",
)
```
</div>

The question-answering pipeline answers questions using information from a given context:

<p>&nbsp;</p>

## Summarization

<div class = "python">
```{python}
summarizer = pipeline("summarization")
summarizer(
    """This paper offers insights into the diffusion and impact of artificial intelligence in science.
More specifically, we show that neural network-based technology meets the essential properties of emerging technologies in the scientific realm.
It is novel, because it shows discontinuous innovations in the originating domain and is put to new uses in many application domains;
it is quick growing, its dimensions being subject to rapid change; it is coherent, because it detaches from its technological parents, and integrates and is accepted in different scientific communities;
and it has a prominent impact on scientific discovery, but a high degree of uncertainty and ambiguity associated with this impact.
Our findings suggest that intelligent machines diffuse in the sciences, reshape the nature of the discovery process and affect the organization of science.
We propose a new conceptual framework that considers artificial intelligence as an emerging general method of invention and, on this basis, derive its policy implications."""
)
```
</div>

Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Hereâ€™s an example:

<p>&nbsp;</p>

# Using specific model

See this **Warning message**: "Using a pipeline without specifying a model name and revision in production is not recommended."
  
  
- **Recommendation**:
  - Specify the model name and revision when using pipelines in production environments.


- **Actionable step**:
  - Choose a specific model from the 1M+ models available on Hugging Face.


- **Resource**:
  - Hugging Face model repository: [https://huggingface.co/models](https://huggingface.co/models)

<div class = "python">
```{python}
generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In my programming course in DS2E I will",
    max_length=30,
    num_return_sequences=3,
)
```
</div>

<div class = "python">
```{python}
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```
</div>

For translation, you can use a default model if you provide a language pair in the task name (such as "translation_en_to_fr"), but the easiest way is to pick the model you want to use on the Model Hub. 

Here weâ€™ll try translating from French to English:

<p>&nbsp;</p>

# Bias and limitations

- Pretrained and fine-tuned models are powerful tools, but they have limitations.


- The main limitation stems from the nature of pretraining on large datasets.
  - Data is often scraped indiscriminately from the internet.
  - This includes both high-quality and low-quality content.


- An example is provided with a fill-mask pipeline using the BERT model.

<div class = "python">
```{python}
unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])
```
</div>

<p>&nbsp;</p>

# Behind the pipeline function

<div class = "python">
```{python} 
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
sentiments = classifier(
        ["I hate teaching",
         "I love programming"]
    )
sentiments
```
</div>

this `pipeline` groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:

<p>&nbsp;</p>

## Preprocessing with a tokenizer

<div class = "python">
```{python}
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
</div>

- **Transformer models** can't process raw text directly.


- The first step is to **convert text inputs into numbers** using a tokenizer.


- The tokenizer is responsible for:
  - **Splitting the input** into tokens (words, subwords, or symbols like punctuation).
  - **Mapping each token to an integer**.
  - **Adding additional inputs** useful to the model.


- Preprocessing must be consistent with how the model was pretrained.


- **AutoTokenizer** class and its **from_pretrained()** method help fetch and cache the tokenizer information.

<div class = "python">
```{python}
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
</div>

- The default checkpoint for the **sentiment-analysis pipeline** is **distilbert-base-uncased-finetuned-sst-2-english**.


- ðŸ¤— Transformers can be used without concern for the underlying ML framework (PyTorch, TensorFlow, or Flax).


- Transformer models require **tensors** as input.


- Tensors are similar to NumPy arrays, which can have:
  - 0D (scalar),
  - 1D (vector),
  - 2D (matrix),
  - or more dimensions.


- Other ML frameworks' tensors behave similarly to NumPy arrays and are easy to instantiate.


<div class = "python">
```{python}
raw_inputs = [
    "I hate teaching",
    "I love programming",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt") # Here pytorch
inputs
```
</div>

- **Output Structure**:
  - A dictionary containing two keys: `input_ids` and `attention_mask`.
  - `input_ids`: Two rows of integers, one for each sentence, representing the unique identifiers of the tokens.
  - `attention_mask`: A tensor with the same shape as the `input_ids`, filled with 0s and 1s, where:
    - 1s indicate tokens to be attended to.
    - 0s indicate tokens to be ignored by the model's attention layers.
    

- **Padding**:
  - Padding ensures all sequences in a batch match the length of the longest sequence by adding a special padding token (e.g., `[PAD]`).
  - Padding is important for:
    - Consistency in sequence length across a batch, ensuring efficient processing.
    - Avoiding model bias caused by sequence length variations, which could affect performance.


- **Example**:
  - Sentences:
    - "I love NLP."
    - "Padding in tokenizers is useful."
  - Maximum sequence length = 5 tokens.
  - The shorter sentence is padded:
    - "I love NLP [PAD] [PAD]"

<p>&nbsp;</p>

## Tokenizers

<p>&nbsp;</p>

### Word-based Tokenizer

<div class = "python">
```{python}
tokenized_text = "tokenize the text into words by applying Pythonâ€™s split() function".split()
tokenized_text
```
</div>

  - Simple and easy to set up with few rules.


  - Yields decent results for many applications.
  
  
- **Goal:**
  - Split raw text into words.
  - Find a numerical representation for each word.


- **Text Splitting Methods:**
  - Can split text in different ways.
  - Example: Using whitespace to tokenize text into words.
  - Python's `split()` function can be used for this purpose.


<div class = "python">
```{python}
tokenized_text = "tokenize the text into words by applying Pythonâ€™s split() function".split()
tokenized_text
```
</div>
- Word tokenizers can include extra rules for punctuation.


- These tokenizers create vocabularies, defined by the total number of independent tokens in the corpus.


- Each word in the corpus is assigned a unique ID, starting from 0, which the model uses to identify each word.


- A comprehensive word-based tokenizer needs an identifier for every word in a language, leading to a large number of tokens.
   - Example: The English language has over 500,000 words, meaning a vast vocabulary and many unique IDs.
   - Words like "dog" and "dogs" or "run" and "running" are seen as unrelated by the model initially, as thereâ€™s no inherent recognition of similarity.


- Tokenizers include an "unknown" token (often `[UNK]` or `<unk>`) for words not in the vocabulary.

 - If many unknown tokens are produced, it indicates that the tokenizer is struggling to represent words accurately, losing information.
 
<p>&nbsp;</p>
    
###  Character-based

**Character-based tokenization** splits text into characters rather than words.
  
  **Primary benefits:**
  - Smaller vocabulary.
  - Fewer out-of-vocabulary (unknown) tokens, as every word can be constructed from characters.
  
  
  **Challenges:**
  - Handling spaces and punctuation can raise issues.
  
  
  **Drawbacks:**
  - Representation may be less meaningful since individual characters carry less information compared to words.
  - This varies across languages (e.g., Chinese characters hold more information than characters in Latin languages).
  - It produces a larger number of tokens to process. A word that is a single token in word-based tokenization may become 10+ tokens in character-based tokenization.
  
<p>&nbsp;</p>

### Subword tokenization

**Subword tokenization** offers a compromise, combining word-based and character-based approaches.

- Subword tokenization algorithms are based on the idea that:
  - Frequently used words should not be split.
  - Rare words should be decomposed into meaningful subwords.
  
  
- Example: 
  - "Annoyingly" could be split into "annoying" and "ly".
  - These subwords are more common and retain the original meaning.
  - `Let's</w> do</w> token ization</w> !</w>`


- Benefits of subword tokenization:
  - Semantic meaning is preserved through subword combinations.
  - Efficient representation of long words using fewer tokens.
  - Achieves good vocabulary coverage.
  - Minimizes unknown tokens.


  ### Loading and saving

<div class = "python">
```{python}
tokenizer.save_pretrained("/Users/peltouz/Documents/pretrain/test")
```
</div>

- Loading and saving tokenizers is similar to handling models.


- It uses the same two methods: `from_pretrained()` and `save_pretrained()`.


- These methods load or save:
  - The algorithm used by the tokenizer (comparable to the architecture of a model).
  - The vocabulary (comparable to the weights of a model).

<div class = "python">
```{python}
tokenizer.save_pretrained("/Users/peltouz/Documents/pretrain/test")
```
</div>
<p>&nbsp;</p>

## Going through the model

<div class = "python">
```{python}
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
</div>

from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)

<div class = "python">
```{python}
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```
</div>

- The outputs of ðŸ¤— Transformers models resemble namedtuples or dictionaries.


- You can access elements in different ways:
  - By attributes (e.g., `outputs.last_hidden_state`)
  - By key (e.g., `outputs["last_hidden_state"]`)
  - By index, if you know the exact position (e.g., `outputs[0]`)

<p>&nbsp;</p>

## Model heads: Making sense out of numbers

  - Take high-dimensional vectors of hidden states as input.
  
  - Project these onto a different dimension.
  
  - Usually composed of one or a few linear layers.


- **Transformers and Model Heads:**
  - The output of the Transformer model is sent directly to the model head for further processing.
  - The embeddings layer in the model converts input IDs into vectors representing the associated tokens.
  - Subsequent layers manipulate these vectors using the attention mechanism to produce final sentence representations.



- **Available Architectures in ðŸ¤— Transformers:**
  - Model (retrieves hidden states)
  - ForCausalLM
  - ForMaskedLM
  - ForMultipleChoice
  - ForQuestionAnswering
  - ForSequenceClassification
  - ForTokenClassification



 - For example for sentence classification tasks, use `AutoModelForSequenceClassification` instead of `AutoModel`.

<div class = "python">
```{python}
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
</div>

- The model head reduces dimensionality by taking high-dimensional vectors as input.


- It outputs vectors containing two values, one for each label.


- Since there are two sentences and two labels, the resulting output has a shape of 2 x 2.


<div class = "python">
```{python}
print(outputs.logits.shape)
print(outputs.logits)
```
</div>

- The values referred to are logits, not probabilities.


- Logits are raw, unnormalized scores outputted by the model's last layer.


- To convert logits to probabilities, they must pass through a SoftMax layer.
  - SoftMax is a generalization of the logistic function to multiple dimensions.
  - It is used in multinomial logistic regression.


During training, the loss function typically combines the final activation function (e.g., SoftMax) with the loss function (e.g., cross entropy).

<div class = "python">
```{python}
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
</div>

- These are recognizable probability scores.

- To get the labels corresponding to each position, we can inspect the id2label attribute of the model config (more on this in the next section):

<div class = "python">
```{python}

model.config.id2label
```
</div>
<p>&nbsp;</p>

# Models

- **AutoModel Class**: 
  - Designed to instantiate any model from a checkpoint.
  - Functions as a wrapper for the various models in the library.
  - Automatically guesses the appropriate model architecture for the checkpoint.
  - Instantiates a model with the guessed architecture.


- **Direct Model Class Usage**:
  - If the model type is known, the specific class defining the architecture can be used directly (e.g., BERT model).

<p>&nbsp;</p>

## Creating a Transformer

The first thing weâ€™ll need to do to initialize a BERT model is load a configuration object:


<div class = "python">
```{python}
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

print(config)
``` 
</div>

- The model can be used in its current state but will output gibberish.


- The model requires training before it can perform well.


- Training the model from scratch would:
  - Take a long time.
  - Require a lot of data.
  - Have a non-negligible environmental impact.


- Instead, reusing pre-trained models can avoid unnecessary and duplicated efforts.


- Pre-trained Transformer models can be easily loaded using the `from_pretrained()` method.


<div class = "python">
```{python}
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```
</div>

- **Using AutoModel**:
  - Replace `BertModel` with `AutoModel` to produce checkpoint-agnostic code.
  - This ensures compatibility with different checkpoints trained for similar tasks, even if the architecture varies.


- **Loading Pretrained Models**:
  - In the example, `BertConfig` was not used; instead, a pretrained model (`bert-base-cased`) was loaded.
  - This specific checkpoint was trained by the original BERT authors, with details available in the model card.


- **Model Initialization and Usage**:
  - The model is initialized with pretrained weights from the checkpoint.
  - It can be used directly for inference or fine-tuned on new tasks.
  - Using pretrained weights helps achieve good results faster than training from scratch.


- **Caching and Customizing Cache Folder**:
  - Weights are downloaded and cached in the default folder `~/.cache/huggingface/transformers`.
  - Cache folder location can be customized by setting the `HF_HOME` environment variable.


- **Model Identifiers**:
  - The identifier used to load the model can be from any compatible model on the Model Hub.
  - A full list of available BERT checkpoints can be found [here](https://huggingface.co/models?other=bert).

<p>&nbsp;</p>

## Saving methods

- Use the `save_pretrained()` method to save a model.

<div class = "python">
```{python}
model.save_pretrained("/Users/peltouz/Documents/pretrain/test")
```
</div>

- **config.json file**:
  - Contains attributes needed to build the model architecture.
  - Includes metadata:
    - Checkpoint origin.
    - ðŸ¤— Transformers version used when the checkpoint was last saved.


- **pytorch_model.bin file**:
  - Known as the state dictionary.
  - Contains all the modelâ€™s weights (parameters).


- **Relationship**:
  - The `config.json` file provides the model architecture.
  - The `pytorch_model.bin` file holds the modelâ€™s parameters (weights).

<p>&nbsp;</p>

#  Wrapping up: From tokenizer to model

<div class = "python">
```{python}

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
</div>


<div class = "python">
```{python}
predictions = torch.nn.functional.softmax(output.logits, dim=-1)
print(predictions)
model.config.id2label
```
</div>